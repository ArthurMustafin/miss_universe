{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1952 (Finland)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7c1c193db741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scraping {name} ({year})...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "        response = requests.get(url, stream=True, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    name, year, _, _, _, _, _, _, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{name.replace(' ', '_')}_{year}.jpg\")\n",
    "        download_image(image_url, image_filename)\n",
    "    else:\n",
    "        print(f\"No image found for {name} ({year})\")\n",
    "\n",
    "\n",
    "    # Extract brief bio\n",
    "    bio_element = soup.find(\"div\", class_=\"reflist\")\n",
    "    bio = \"\"\n",
    "    if bio_element:\n",
    "        for elem in bio_element.previous_siblings:\n",
    "            if elem.name == \"p\":\n",
    "                bio = elem.text.strip()\n",
    "                break\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Year', 'Country', 'Location', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1952 (Finland)...\n",
      "Scraping 1953 (France)...\n",
      "Scraping 1954 (United States)...\n",
      "Scraping 1955 (Sweden)...\n",
      "Scraping 1956 (United States)...\n",
      "Scraping 1957 (Peru)...\n",
      "Scraping 1958 (Colombia)...\n",
      "Scraping 1959 (Japan)...\n",
      "Scraping 1960 (United States)...\n",
      "Scraping 1961 (Germany)...\n",
      "No image found for 1961 (Germany)\n",
      "Scraping 1962 (Argentina)...\n",
      "No image found for 1962 (Argentina)\n",
      "Scraping 1963 (Brazil)...\n",
      "Scraping 1964 (Greece)...\n",
      "No image found for 1964 (Greece)\n",
      "Scraping 1965 (Thailand)...\n",
      "No image found for 1965 (Thailand)\n",
      "Scraping 1966 (Sweden)...\n",
      "No image found for 1966 (Sweden)\n",
      "Scraping 1967 (United States)...\n",
      "Scraping 1968 (Brazil)...\n",
      "Scraping 1969 (Philippines)...\n",
      "Scraping 1970 (Puerto Rico)...\n",
      "Scraping 1971 (Lebanon)...\n",
      "Scraping 1972 (Australia)...\n",
      "No image found for 1972 (Australia)\n",
      "Scraping 1973 (Philippines)...\n",
      "Scraping 1974 (Spain)...\n",
      "No image found for 1974 (Spain)\n",
      "Scraping 1975 (Finland)...\n",
      "No image found for 1975 (Finland)\n",
      "Scraping 1976 (Israel)...\n",
      "Scraping 1977 (Trinidad and Tobago)...\n",
      "Scraping 1978 (South Africa)...\n",
      "Scraping 1979 (Venezuela)...\n",
      "Scraping 1980 (United States)...\n",
      "Scraping 1981 (Venezuela)...\n",
      "Scraping 1982 (Canada)...\n",
      "No image found for 1982 (Canada)\n",
      "Scraping 1983 (New Zealand)...\n",
      "Scraping 1984 (Sweden)...\n",
      "Scraping 1985 (Puerto Rico)...\n",
      "No image found for 1985 (Puerto Rico)\n",
      "Scraping 1986 (Venezuela)...\n",
      "Scraping 1987 (Chile)...\n",
      "Scraping 1988 (Thailand)...\n",
      "No image found for 1988 (Thailand)\n",
      "Scraping 1989 (Netherlands)...\n",
      "Scraping 1990 (Norway)...\n",
      "No image found for 1990 (Norway)\n",
      "Scraping 1991 (Mexico)...\n",
      "Scraping 1992 (Namibia)...\n",
      "No image found for 1992 (Namibia)\n",
      "Scraping 1993 (Puerto Rico)...\n",
      "Scraping 1994 (India)...\n",
      "Scraping 1995 (United States)...\n",
      "Scraping 1996 (Venezuela)...\n",
      "Scraping 1997 (United States)...\n",
      "Scraping 1998 (Trinidad and Tobago)...\n",
      "Scraping 1999 (Botswana)...\n",
      "Scraping 2000 (India)...\n",
      "Scraping 2001 (Puerto Rico)...\n",
      "Scraping 2002 (Russia)...\n",
      "Scraping 2002 (Panama)...\n",
      "Scraping 2003 (Dominican Republic)...\n",
      "Scraping 2004 (Australia)...\n",
      "Scraping 2005 (Canada)...\n",
      "Scraping 2006 (Puerto Rico)...\n",
      "Scraping 2007 (Japan)...\n",
      "Scraping 2008 (Venezuela)...\n",
      "Scraping 2009 (Venezuela)...\n",
      "Scraping 2010 (Mexico)...\n",
      "Scraping 2011 (Angola)...\n",
      "Scraping 2012 (United States)...\n",
      "Scraping 2013 (Venezuela)...\n",
      "Scraping 2014 (Colombia)...\n",
      "Scraping 2015 (Philippines)...\n",
      "Scraping 2016 (France)...\n",
      "Scraping 2017 (South Africa)...\n",
      "Scraping 2018 (Philippines)...\n",
      "Scraping 2019 (South Africa)...\n",
      "Scraping 2020 (Mexico)...\n",
      "Scraping 2021 (India)...\n",
      "Scraping 2022 (United States)...\n",
      "No image found for 2022 (United States)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    name, year, _, _, _, _, _, _, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{name.replace(' ', '_')}_{year}.jpg\")\n",
    "        download_image(image_url, image_filename)\n",
    "    else:\n",
    "        print(f\"No image found for {name} ({year})\")\n",
    "\n",
    "    # Extract brief bio\n",
    "    bio_element = soup.find(\"div\", class_=\"reflist\")\n",
    "    bio = \"\"\n",
    "    if bio_element:\n",
    "        for elem in bio_element.previous_siblings:\n",
    "            if elem.name == \"p\":\n",
    "                bio = elem.text.strip()\n",
    "                break\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Year', 'Country', 'Location', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Armi Kuusela[1] (1952)...\n",
      "Scraping Christiane Martel[2] (1953)...\n",
      "Scraping Miriam Stevenson[3] (1954)...\n",
      "Scraping Hillevi Rombin † [b][4][5] (1955)...\n",
      "Scraping Carol Morris[6] (1956)...\n",
      "Scraping Gladys Zender[7] (1957)...\n",
      "Scraping Luz Marina Zuluaga † [b][8] (1958)...\n",
      "Scraping Akiko Kojima[9] (1959)...\n",
      "Scraping Linda Bement † [b][10] (1960)...\n",
      "Scraping Marlene Schmidt[11] (1961)...\n",
      "No image found for Marlene Schmidt[11] (1961)\n",
      "Scraping Norma Nolan[12] (1962)...\n",
      "No image found for Norma Nolan[12] (1962)\n",
      "Scraping Iêda Maria Vargas[13] (1963)...\n",
      "Scraping Corinna Tsopei[14] (1964)...\n",
      "No image found for Corinna Tsopei[14] (1964)\n",
      "Scraping Apasra Hongsakula[15] (1965)...\n",
      "No image found for Apasra Hongsakula[15] (1965)\n",
      "Scraping Margareta Arvidsson[16] (1966)...\n",
      "No image found for Margareta Arvidsson[16] (1966)\n",
      "Scraping Sylvia Hitchcock † [b][17] (1967)...\n",
      "Scraping Martha Vasconcellos[18] (1968)...\n",
      "Scraping Gloria Diaz[19] (1969)...\n",
      "Scraping Marisol Malaret[20] (1970)...\n",
      "Scraping Georgina Rizk[21] (1971)...\n",
      "Scraping Kerry Anne Wells[22] (1972)...\n",
      "No image found for Kerry Anne Wells[22] (1972)\n",
      "Scraping Margie Moran[23] (1973)...\n",
      "Scraping Amparo Muñoz † [b][24] (1974)...\n",
      "No image found for Amparo Muñoz † [b][24] (1974)\n",
      "Scraping Anne Marie Pohtamo[25] (1975)...\n",
      "No image found for Anne Marie Pohtamo[25] (1975)\n",
      "Scraping Rina Messinger[26] (1976)...\n",
      "Scraping Janelle Commissiong[27] (1977)...\n",
      "Scraping Margaret Gardiner[28] (1978)...\n",
      "Scraping Maritza Sayalero[29] (1979)...\n",
      "Scraping Shawn Weatherly[30] (1980)...\n",
      "Scraping Irene Sáez[31] (1981)...\n",
      "Scraping Karen Baldwin[32] (1982)...\n",
      "No image found for Karen Baldwin[32] (1982)\n",
      "Scraping Lorraine Downes[33] (1983)...\n",
      "Scraping Yvonne Ryding[34] (1984)...\n",
      "Scraping Deborah Carthy-Deu[35] (1985)...\n",
      "No image found for Deborah Carthy-Deu[35] (1985)\n",
      "Scraping Bárbara Palacios[36] (1986)...\n",
      "Scraping Cecilia Bolocco[37] (1987)...\n",
      "Scraping Porntip Nakhirunkanok[38] (1988)...\n",
      "No image found for Porntip Nakhirunkanok[38] (1988)\n",
      "Scraping Angela Visser[39] (1989)...\n",
      "Scraping Mona Grudt[40] (1990)...\n",
      "No image found for Mona Grudt[40] (1990)\n",
      "Scraping Lupita Jones[41] (1991)...\n",
      "Scraping Michelle McLean[42] (1992)...\n",
      "No image found for Michelle McLean[42] (1992)\n",
      "Scraping Dayanara Torres[43] (1993)...\n",
      "Scraping Sushmita Sen[44] (1994)...\n",
      "Scraping Chelsi Smith † [b][45] (1995)...\n",
      "Scraping Alicia Machado[46] (1996)...\n",
      "Scraping Brook Lee[47] (1997)...\n",
      "Scraping Wendy Fitzwilliam[48] (1998)...\n",
      "Scraping Mpule Kwelagobe (1999)...\n",
      "Scraping Lara Dutta[49] (2000)...\n",
      "Scraping Denise Quiñones[50] (2001)...\n",
      "Scraping Oxana Fedorova[c] (2002)...\n",
      "Scraping Justine Pasek[d][51] (2002)...\n",
      "Scraping Amelia Vega[52] (2003)...\n",
      "Scraping Jennifer Hawkins[53] (2004)...\n",
      "Scraping Natalie Glebova[54] (2005)...\n",
      "Scraping Zuleyka Rivera[55] (2006)...\n",
      "Scraping Riyo Mori[56] (2007)...\n",
      "Scraping Dayana Mendoza[57] (2008)...\n",
      "Scraping Stefanía Fernández (2009)...\n",
      "Scraping Ximena Navarrete (2010)...\n",
      "Scraping Leila Lopes (2011)...\n",
      "Scraping Olivia Culpo (2012)...\n",
      "Scraping Gabriela Isler (2013)...\n",
      "Scraping Paulina Vega (2014)...\n",
      "Scraping Pia Wurtzbach[58] (2015)...\n",
      "Scraping Iris Mittenaere[59] (2016)...\n",
      "Scraping Demi-Leigh Nel-Peters (2017)...\n",
      "Scraping Catriona Gray (2018)...\n",
      "Scraping Zozibini Tunzi (2019)...\n",
      "Scraping Andrea Meza (2020)...\n",
      "Scraping Harnaaz Sandhu[60] (2021)...\n",
      "Scraping R'Bonney Gabriel (2022)...\n",
      "No image found for R'Bonney Gabriel (2022)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    year, country, name, age, hometown, national_title, date, entrants, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{name.replace(' ', '_')}_{year}.jpg\")\n",
    "        download_image(image_url, image_filename)\n",
    "    else:\n",
    "        print(f\"No image found for {name} ({year})\")\n",
    "\n",
    "    # Extract brief bio from the \"(Top)\" section of the page\n",
    "    bio = \"\"\n",
    "    first_heading = soup.find(\"h2\")\n",
    "    if first_heading:\n",
    "        for elem in first_heading.previous_siblings:\n",
    "            if elem.name == \"p\":\n",
    "                bio = elem.text.strip()\n",
    "                break\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Year', 'Country', 'Name', 'Age', 'Hometown', 'National Title', 'Date', 'Entrants', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Armi Kuusela[1] (1952)...\n",
      "Scraping Christiane Martel[2] (1953)...\n",
      "Scraping Miriam Stevenson[3] (1954)...\n",
      "Scraping Hillevi Rombin † [b][4][5] (1955)...\n",
      "Scraping Carol Morris[6] (1956)...\n",
      "Scraping Gladys Zender[7] (1957)...\n",
      "Scraping Luz Marina Zuluaga † [b][8] (1958)...\n",
      "Scraping Akiko Kojima[9] (1959)...\n",
      "Scraping Linda Bement † [b][10] (1960)...\n",
      "Scraping Marlene Schmidt[11] (1961)...\n",
      "No image found for Marlene Schmidt[11] (1961)\n",
      "Scraping Norma Nolan[12] (1962)...\n",
      "No image found for Norma Nolan[12] (1962)\n",
      "Scraping Iêda Maria Vargas[13] (1963)...\n",
      "Scraping Corinna Tsopei[14] (1964)...\n",
      "No image found for Corinna Tsopei[14] (1964)\n",
      "Scraping Apasra Hongsakula[15] (1965)...\n",
      "No image found for Apasra Hongsakula[15] (1965)\n",
      "Scraping Margareta Arvidsson[16] (1966)...\n",
      "No image found for Margareta Arvidsson[16] (1966)\n",
      "Scraping Sylvia Hitchcock † [b][17] (1967)...\n",
      "Scraping Martha Vasconcellos[18] (1968)...\n",
      "Scraping Gloria Diaz[19] (1969)...\n",
      "Scraping Marisol Malaret[20] (1970)...\n",
      "Scraping Georgina Rizk[21] (1971)...\n",
      "Scraping Kerry Anne Wells[22] (1972)...\n",
      "No image found for Kerry Anne Wells[22] (1972)\n",
      "Scraping Margie Moran[23] (1973)...\n",
      "Scraping Amparo Muñoz † [b][24] (1974)...\n",
      "No image found for Amparo Muñoz † [b][24] (1974)\n",
      "Scraping Anne Marie Pohtamo[25] (1975)...\n",
      "No image found for Anne Marie Pohtamo[25] (1975)\n",
      "Scraping Rina Messinger[26] (1976)...\n",
      "Scraping Janelle Commissiong[27] (1977)...\n",
      "Scraping Margaret Gardiner[28] (1978)...\n",
      "Scraping Maritza Sayalero[29] (1979)...\n",
      "Scraping Shawn Weatherly[30] (1980)...\n",
      "Scraping Irene Sáez[31] (1981)...\n",
      "Scraping Karen Baldwin[32] (1982)...\n",
      "No image found for Karen Baldwin[32] (1982)\n",
      "Scraping Lorraine Downes[33] (1983)...\n",
      "Scraping Yvonne Ryding[34] (1984)...\n",
      "Scraping Deborah Carthy-Deu[35] (1985)...\n",
      "No image found for Deborah Carthy-Deu[35] (1985)\n",
      "Scraping Bárbara Palacios[36] (1986)...\n",
      "Scraping Cecilia Bolocco[37] (1987)...\n",
      "Scraping Porntip Nakhirunkanok[38] (1988)...\n",
      "No image found for Porntip Nakhirunkanok[38] (1988)\n",
      "Scraping Angela Visser[39] (1989)...\n",
      "Scraping Mona Grudt[40] (1990)...\n",
      "No image found for Mona Grudt[40] (1990)\n",
      "Scraping Lupita Jones[41] (1991)...\n",
      "Scraping Michelle McLean[42] (1992)...\n",
      "No image found for Michelle McLean[42] (1992)\n",
      "Scraping Dayanara Torres[43] (1993)...\n",
      "Scraping Sushmita Sen[44] (1994)...\n",
      "Scraping Chelsi Smith † [b][45] (1995)...\n",
      "Scraping Alicia Machado[46] (1996)...\n",
      "Scraping Brook Lee[47] (1997)...\n",
      "Scraping Wendy Fitzwilliam[48] (1998)...\n",
      "Scraping Mpule Kwelagobe (1999)...\n",
      "Scraping Lara Dutta[49] (2000)...\n",
      "Scraping Denise Quiñones[50] (2001)...\n",
      "Scraping Oxana Fedorova[c] (2002)...\n",
      "Scraping Justine Pasek[d][51] (2002)...\n",
      "Scraping Amelia Vega[52] (2003)...\n",
      "Scraping Jennifer Hawkins[53] (2004)...\n",
      "Scraping Natalie Glebova[54] (2005)...\n",
      "Scraping Zuleyka Rivera[55] (2006)...\n",
      "Scraping Riyo Mori[56] (2007)...\n",
      "Scraping Dayana Mendoza[57] (2008)...\n",
      "Scraping Stefanía Fernández (2009)...\n",
      "Scraping Ximena Navarrete (2010)...\n",
      "Scraping Leila Lopes (2011)...\n",
      "Scraping Olivia Culpo (2012)...\n",
      "Scraping Gabriela Isler (2013)...\n",
      "Scraping Paulina Vega (2014)...\n",
      "Scraping Pia Wurtzbach[58] (2015)...\n",
      "Scraping Iris Mittenaere[59] (2016)...\n",
      "Scraping Demi-Leigh Nel-Peters (2017)...\n",
      "Scraping Catriona Gray (2018)...\n",
      "Scraping Zozibini Tunzi (2019)...\n",
      "Scraping Andrea Meza (2020)...\n",
      "Scraping Harnaaz Sandhu[60] (2021)...\n",
      "Scraping R'Bonney Gabriel (2022)...\n",
      "No image found for R'Bonney Gabriel (2022)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    year, country, name, age, hometown, national_title, date, entrants, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{name.replace(' ', '_')}_{year}.jpg\")\n",
    "        download_image(image_url, image_filename)\n",
    "    else:\n",
    "        print(f\"No image found for {name} ({year})\")\n",
    "\n",
    "    # Extract brief bio from the top section of the page\n",
    "    bio = \"\"\n",
    "    first_heading = soup.find(\"h2\")\n",
    "    if first_heading:\n",
    "        for elem in reversed(first_heading.find_all_previous(\"p\")):\n",
    "            if elem.text.strip():\n",
    "                bio = elem.text.strip()\n",
    "                break\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Year', 'Country', 'Name', 'Age', 'Hometown', 'National Title', 'Date', 'Entrants', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Armi Kuusela[1] (1952)...\n",
      "Scraping Christiane Martel[2] (1953)...\n",
      "Scraping Miriam Stevenson[3] (1954)...\n",
      "Scraping Hillevi Rombin † [b][4][5] (1955)...\n",
      "Scraping Carol Morris[6] (1956)...\n",
      "Scraping Gladys Zender[7] (1957)...\n",
      "Scraping Luz Marina Zuluaga † [b][8] (1958)...\n",
      "Scraping Akiko Kojima[9] (1959)...\n",
      "Scraping Linda Bement † [b][10] (1960)...\n",
      "Scraping Marlene Schmidt[11] (1961)...\n",
      "Scraping Norma Nolan[12] (1962)...\n",
      "Scraping Iêda Maria Vargas[13] (1963)...\n",
      "Scraping Corinna Tsopei[14] (1964)...\n",
      "Scraping Apasra Hongsakula[15] (1965)...\n",
      "Scraping Margareta Arvidsson[16] (1966)...\n",
      "Scraping Sylvia Hitchcock † [b][17] (1967)...\n",
      "Scraping Martha Vasconcellos[18] (1968)...\n",
      "Scraping Gloria Diaz[19] (1969)...\n",
      "Scraping Marisol Malaret[20] (1970)...\n",
      "Scraping Georgina Rizk[21] (1971)...\n",
      "Scraping Kerry Anne Wells[22] (1972)...\n",
      "Scraping Margie Moran[23] (1973)...\n",
      "Scraping Amparo Muñoz † [b][24] (1974)...\n",
      "Scraping Anne Marie Pohtamo[25] (1975)...\n",
      "Scraping Rina Messinger[26] (1976)...\n",
      "Scraping Janelle Commissiong[27] (1977)...\n",
      "Scraping Margaret Gardiner[28] (1978)...\n",
      "Scraping Maritza Sayalero[29] (1979)...\n",
      "Scraping Shawn Weatherly[30] (1980)...\n",
      "Scraping Irene Sáez[31] (1981)...\n",
      "Scraping Karen Baldwin[32] (1982)...\n",
      "Scraping Lorraine Downes[33] (1983)...\n",
      "Scraping Yvonne Ryding[34] (1984)...\n",
      "Scraping Deborah Carthy-Deu[35] (1985)...\n",
      "Scraping Bárbara Palacios[36] (1986)...\n",
      "Scraping Cecilia Bolocco[37] (1987)...\n",
      "Scraping Porntip Nakhirunkanok[38] (1988)...\n",
      "Scraping Angela Visser[39] (1989)...\n",
      "Scraping Mona Grudt[40] (1990)...\n",
      "Scraping Lupita Jones[41] (1991)...\n",
      "Scraping Michelle McLean[42] (1992)...\n",
      "Scraping Dayanara Torres[43] (1993)...\n",
      "Scraping Sushmita Sen[44] (1994)...\n",
      "Scraping Chelsi Smith † [b][45] (1995)...\n",
      "Scraping Alicia Machado[46] (1996)...\n",
      "Scraping Brook Lee[47] (1997)...\n",
      "Scraping Wendy Fitzwilliam[48] (1998)...\n",
      "Scraping Mpule Kwelagobe (1999)...\n",
      "Scraping Lara Dutta[49] (2000)...\n",
      "Scraping Denise Quiñones[50] (2001)...\n",
      "Scraping Oxana Fedorova[c] (2002)...\n",
      "Scraping Justine Pasek[d][51] (2002)...\n",
      "Scraping Amelia Vega[52] (2003)...\n",
      "Scraping Jennifer Hawkins[53] (2004)...\n",
      "Scraping Natalie Glebova[54] (2005)...\n",
      "Scraping Zuleyka Rivera[55] (2006)...\n",
      "Scraping Riyo Mori[56] (2007)...\n",
      "Scraping Dayana Mendoza[57] (2008)...\n",
      "Scraping Stefanía Fernández (2009)...\n",
      "Scraping Ximena Navarrete (2010)...\n",
      "Scraping Leila Lopes (2011)...\n",
      "Scraping Olivia Culpo (2012)...\n",
      "Scraping Gabriela Isler (2013)...\n",
      "Scraping Paulina Vega (2014)...\n",
      "Scraping Pia Wurtzbach[58] (2015)...\n",
      "Scraping Iris Mittenaere[59] (2016)...\n",
      "Scraping Demi-Leigh Nel-Peters (2017)...\n",
      "Scraping Catriona Gray (2018)...\n",
      "Scraping Zozibini Tunzi (2019)...\n",
      "Scraping Andrea Meza (2020)...\n",
      "Scraping Harnaaz Sandhu[60] (2021)...\n",
      "Scraping R'Bonney Gabriel (2022)...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    year, _, name, _, _, _, _, _, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{year}_{name.replace(' ', '_')}.jpg\")\n",
    "        download_image(image_url, image_filename)\n",
    "\n",
    "    # Extract brief bio\n",
    "    bio_element = soup.find(\"p\")\n",
    "    bio = \"\"\n",
    "    if bio_element:\n",
    "        bio = bio_element.text.strip()\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Year', 'Country', 'Name', 'Age', 'Hometown', 'National Title', 'Date', 'Entrants', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Armi Kuusela[1] (1952)...\n",
      "Scraping Christiane Martel[2] (1953)...\n",
      "Scraping Miriam Stevenson[3] (1954)...\n",
      "Scraping Hillevi Rombin † [b][4][5] (1955)...\n",
      "Scraping Carol Morris[6] (1956)...\n",
      "Scraping Gladys Zender[7] (1957)...\n",
      "Scraping Luz Marina Zuluaga † [b][8] (1958)...\n",
      "Scraping Akiko Kojima[9] (1959)...\n",
      "Scraping Linda Bement † [b][10] (1960)...\n",
      "Scraping Marlene Schmidt[11] (1961)...\n",
      "Scraping Norma Nolan[12] (1962)...\n",
      "Scraping Iêda Maria Vargas[13] (1963)...\n",
      "Scraping Corinna Tsopei[14] (1964)...\n",
      "Scraping Apasra Hongsakula[15] (1965)...\n",
      "Scraping Margareta Arvidsson[16] (1966)...\n",
      "Scraping Sylvia Hitchcock † [b][17] (1967)...\n",
      "Scraping Martha Vasconcellos[18] (1968)...\n",
      "Scraping Gloria Diaz[19] (1969)...\n",
      "Scraping Marisol Malaret[20] (1970)...\n",
      "Scraping Georgina Rizk[21] (1971)...\n",
      "Scraping Kerry Anne Wells[22] (1972)...\n",
      "Scraping Margie Moran[23] (1973)...\n",
      "Scraping Amparo Muñoz † [b][24] (1974)...\n",
      "Scraping Anne Marie Pohtamo[25] (1975)...\n",
      "Scraping Rina Messinger[26] (1976)...\n",
      "Scraping Janelle Commissiong[27] (1977)...\n",
      "Scraping Margaret Gardiner[28] (1978)...\n",
      "Scraping Maritza Sayalero[29] (1979)...\n",
      "Scraping Shawn Weatherly[30] (1980)...\n",
      "Scraping Irene Sáez[31] (1981)...\n",
      "Scraping Karen Baldwin[32] (1982)...\n",
      "Scraping Lorraine Downes[33] (1983)...\n",
      "Scraping Yvonne Ryding[34] (1984)...\n",
      "Scraping Deborah Carthy-Deu[35] (1985)...\n",
      "Scraping Bárbara Palacios[36] (1986)...\n",
      "Scraping Cecilia Bolocco[37] (1987)...\n",
      "Scraping Porntip Nakhirunkanok[38] (1988)...\n",
      "Scraping Angela Visser[39] (1989)...\n",
      "Scraping Mona Grudt[40] (1990)...\n",
      "Scraping Lupita Jones[41] (1991)...\n",
      "Scraping Michelle McLean[42] (1992)...\n",
      "Scraping Dayanara Torres[43] (1993)...\n",
      "Scraping Sushmita Sen[44] (1994)...\n",
      "Scraping Chelsi Smith † [b][45] (1995)...\n",
      "Scraping Alicia Machado[46] (1996)...\n",
      "Scraping Brook Lee[47] (1997)...\n",
      "Scraping Wendy Fitzwilliam[48] (1998)...\n",
      "Scraping Mpule Kwelagobe (1999)...\n",
      "Scraping Lara Dutta[49] (2000)...\n",
      "Scraping Denise Quiñones[50] (2001)...\n",
      "Scraping Oxana Fedorova[c] (2002)...\n",
      "Scraping Justine Pasek[d][51] (2002)...\n",
      "Scraping Amelia Vega[52] (2003)...\n",
      "Scraping Jennifer Hawkins[53] (2004)...\n",
      "Scraping Natalie Glebova[54] (2005)...\n",
      "Scraping Zuleyka Rivera[55] (2006)...\n",
      "Scraping Riyo Mori[56] (2007)...\n",
      "Scraping Dayana Mendoza[57] (2008)...\n",
      "Scraping Stefanía Fernández (2009)...\n",
      "Scraping Ximena Navarrete (2010)...\n",
      "Scraping Leila Lopes (2011)...\n",
      "Scraping Olivia Culpo (2012)...\n",
      "Scraping Gabriela Isler (2013)...\n",
      "Scraping Paulina Vega (2014)...\n",
      "Scraping Pia Wurtzbach[58] (2015)...\n",
      "Scraping Iris Mittenaere[59] (2016)...\n",
      "Scraping Demi-Leigh Nel-Peters (2017)...\n",
      "Scraping Catriona Gray (2018)...\n",
      "Scraping Zozibini Tunzi (2019)...\n",
      "Scraping Andrea Meza (2020)...\n",
      "Scraping Harnaaz Sandhu[60] (2021)...\n",
      "Scraping R'Bonney Gabriel (2022)...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    year, _, name, _, _, _, _, _, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{year}_{name.replace(' ', '_')}.jpg\")\n",
    "        download_image(image_url, image_filename)\n",
    "\n",
    "    # Extract brief bio\n",
    "    toc_element = soup.find(\"div\", {\"id\": \"toc\"})\n",
    "    if toc_element:\n",
    "        bio_element = toc_element.find_next(\"p\")\n",
    "    else:\n",
    "        bio_element = soup.find(\"p\")\n",
    "\n",
    "    bio = \"\"\n",
    "    if bio_element:\n",
    "        bio = bio_element.text.strip()\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Year', 'Country', 'Name', 'Age', 'Hometown', 'National Title', 'Date', 'Entrants', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Armi Kuusela[1] (1952)...\n",
      "Scraping Christiane Martel[2] (1953)...\n",
      "Scraping Miriam Stevenson[3] (1954)...\n",
      "Scraping Hillevi Rombin † [b][4][5] (1955)...\n",
      "Scraping Carol Morris[6] (1956)...\n",
      "Scraping Gladys Zender[7] (1957)...\n",
      "Scraping Luz Marina Zuluaga † [b][8] (1958)...\n",
      "Scraping Akiko Kojima[9] (1959)...\n",
      "Scraping Linda Bement † [b][10] (1960)...\n",
      "Scraping Marlene Schmidt[11] (1961)...\n",
      "Scraping Norma Nolan[12] (1962)...\n",
      "Scraping Iêda Maria Vargas[13] (1963)...\n",
      "Scraping Corinna Tsopei[14] (1964)...\n",
      "Scraping Apasra Hongsakula[15] (1965)...\n",
      "Scraping Margareta Arvidsson[16] (1966)...\n",
      "Scraping Sylvia Hitchcock † [b][17] (1967)...\n",
      "Scraping Martha Vasconcellos[18] (1968)...\n",
      "Scraping Gloria Diaz[19] (1969)...\n",
      "Scraping Marisol Malaret[20] (1970)...\n",
      "Scraping Georgina Rizk[21] (1971)...\n",
      "Scraping Kerry Anne Wells[22] (1972)...\n",
      "Scraping Margie Moran[23] (1973)...\n",
      "Scraping Amparo Muñoz † [b][24] (1974)...\n",
      "Scraping Anne Marie Pohtamo[25] (1975)...\n",
      "Scraping Rina Messinger[26] (1976)...\n",
      "Scraping Janelle Commissiong[27] (1977)...\n",
      "Scraping Margaret Gardiner[28] (1978)...\n",
      "Scraping Maritza Sayalero[29] (1979)...\n",
      "Scraping Shawn Weatherly[30] (1980)...\n",
      "Scraping Irene Sáez[31] (1981)...\n",
      "Scraping Karen Baldwin[32] (1982)...\n",
      "Scraping Lorraine Downes[33] (1983)...\n",
      "Scraping Yvonne Ryding[34] (1984)...\n",
      "Scraping Deborah Carthy-Deu[35] (1985)...\n",
      "Scraping Bárbara Palacios[36] (1986)...\n",
      "Scraping Cecilia Bolocco[37] (1987)...\n",
      "Scraping Porntip Nakhirunkanok[38] (1988)...\n",
      "Scraping Angela Visser[39] (1989)...\n",
      "Scraping Mona Grudt[40] (1990)...\n",
      "Scraping Lupita Jones[41] (1991)...\n",
      "Scraping Michelle McLean[42] (1992)...\n",
      "Scraping Dayanara Torres[43] (1993)...\n",
      "Scraping Sushmita Sen[44] (1994)...\n",
      "Scraping Chelsi Smith † [b][45] (1995)...\n",
      "Scraping Alicia Machado[46] (1996)...\n",
      "Scraping Brook Lee[47] (1997)...\n",
      "Scraping Wendy Fitzwilliam[48] (1998)...\n",
      "Scraping Mpule Kwelagobe (1999)...\n",
      "Scraping Lara Dutta[49] (2000)...\n",
      "Scraping Denise Quiñones[50] (2001)...\n",
      "Scraping Oxana Fedorova[c] (2002)...\n",
      "Scraping Justine Pasek[d][51] (2002)...\n",
      "Scraping Amelia Vega[52] (2003)...\n",
      "Scraping Jennifer Hawkins[53] (2004)...\n",
      "Scraping Natalie Glebova[54] (2005)...\n",
      "Scraping Zuleyka Rivera[55] (2006)...\n",
      "Scraping Riyo Mori[56] (2007)...\n",
      "Scraping Dayana Mendoza[57] (2008)...\n",
      "Scraping Stefanía Fernández (2009)...\n",
      "Scraping Ximena Navarrete (2010)...\n",
      "Scraping Leila Lopes (2011)...\n",
      "Scraping Olivia Culpo (2012)...\n",
      "Scraping Gabriela Isler (2013)...\n",
      "Scraping Paulina Vega (2014)...\n",
      "Scraping Pia Wurtzbach[58] (2015)...\n",
      "Scraping Iris Mittenaere[59] (2016)...\n",
      "Scraping Demi-Leigh Nel-Peters (2017)...\n",
      "Scraping Catriona Gray (2018)...\n",
      "Scraping Zozibini Tunzi (2019)...\n",
      "Scraping Andrea Meza (2020)...\n",
      "Scraping Harnaaz Sandhu[60] (2021)...\n",
      "Scraping R'Bonney Gabriel (2022)...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def find_first_non_empty_paragraph(soup):\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.text.strip():\n",
    "            return paragraph\n",
    "    return None\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    year, _, name, _, _, _, _, _, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "\n",
    "        # Get the URL of the larger image\n",
    "        image_page_url = \"https://en.wikipedia.org\" + image_element.parent[\"href\"]\n",
    "        image_page_response = requests.get(image_page_url, headers=headers)\n",
    "        image_page_soup = BeautifulSoup(image_page_response.content, \"html.parser\")\n",
    "        original_image_element = image_page_soup.find(\"div\", class_=\"fullImageLink\").find(\"a\")\n",
    "        if original_image_element:\n",
    "            large_image_url = \"https:\" + original_image_element[\"href\"]\n",
    "\n",
    "            image_filename = os.path.join(\"images\", f\"{year}_{name.replace(' ', '_')}.jpg\")\n",
    "            if not download_image(large_image_url, image_filename):\n",
    "                download_image(image_url, image_filename)\n",
    "\n",
    "    # Extract brief bio\n",
    "    bio_element = find_first_non_empty_paragraph(soup)\n",
    "    bio = \"\"\n",
    "    if bio_element:\n",
    "        bio = bio_element.text.strip()\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Year', 'Country', 'Name', 'Age', 'Hometown', 'National Title', 'Date', 'Entrants', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Armi Kuusela[1] (1952)...\n",
      "Scraping Christiane Martel[2] (1953)...\n",
      "Scraping Miriam Stevenson[3] (1954)...\n",
      "Scraping Hillevi Rombin † [b][4][5] (1955)...\n",
      "Scraping Carol Morris[6] (1956)...\n",
      "Scraping Gladys Zender[7] (1957)...\n",
      "Scraping Luz Marina Zuluaga † [b][8] (1958)...\n",
      "Scraping Akiko Kojima[9] (1959)...\n",
      "Scraping Linda Bement † [b][10] (1960)...\n",
      "Scraping Marlene Schmidt[11] (1961)...\n",
      "Scraping Norma Nolan[12] (1962)...\n",
      "Scraping Iêda Maria Vargas[13] (1963)...\n",
      "Scraping Corinna Tsopei[14] (1964)...\n",
      "Scraping Apasra Hongsakula[15] (1965)...\n",
      "Scraping Margareta Arvidsson[16] (1966)...\n",
      "Scraping Sylvia Hitchcock † [b][17] (1967)...\n",
      "Scraping Martha Vasconcellos[18] (1968)...\n",
      "Scraping Gloria Diaz[19] (1969)...\n",
      "Scraping Marisol Malaret[20] (1970)...\n",
      "Scraping Georgina Rizk[21] (1971)...\n",
      "Scraping Kerry Anne Wells[22] (1972)...\n",
      "Scraping Margie Moran[23] (1973)...\n",
      "Scraping Amparo Muñoz † [b][24] (1974)...\n",
      "Scraping Anne Marie Pohtamo[25] (1975)...\n",
      "Scraping Rina Messinger[26] (1976)...\n",
      "Scraping Janelle Commissiong[27] (1977)...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot write mode RGBA as JPEG",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAWMODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RGBA'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5d1ba8c004f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{year} - {name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{year}_{name.replace(' ', '_')}_with_text.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0madd_text_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Extract brief bio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c5d1ba8c004f>\u001b[0m in \u001b[0;36madd_text_to_image\u001b[0;34m(image_path, text, output_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Save the modified image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Scrape individual titleholder pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2152\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAWMODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot write mode {im.mode} as JPEG\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: cannot write mode RGBA as JPEG"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Read the CSV file\n",
    "with open('miss_universe_titleholders.csv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    titleholders = [row for row in reader]\n",
    "\n",
    "# Base URL for Wikipedia\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Set up headers with custom User-Agent\n",
    "headers = {'User-Agent': 'MissUniverseScraper/1.0 (https://github.com/ArthurMustafin/miss_universe; ksa8art@gmail.com)'}\n",
    "\n",
    "# Create a directory to store the images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Define a function to download and save images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def add_text_to_image(image_path, text, output_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Create a draw object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Set the font\n",
    "    font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 20)  # Set the font and size here\n",
    " \n",
    "    # Calculate text position (bottom left corner)\n",
    "    text_position = (10, image.height - 30) # You can adjust the position here\n",
    "\n",
    "    # Draw text on the image\n",
    "    draw.text(text_position, text, font=font, fill=\"white\")\n",
    "\n",
    "    # Save the modified image\n",
    "    image.save(output_path)\n",
    "\n",
    "# Scrape individual titleholder pages\n",
    "for titleholder in titleholders:\n",
    "    year, _, name, _, _, _, _, _, page_url = titleholder\n",
    "\n",
    "    print(f\"Scraping {name} ({year})...\")\n",
    "\n",
    "    response = requests.get(base_url + page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Download main image\n",
    "    image_element = soup.find(\"table\", class_=\"infobox\").find(\"img\")\n",
    "    if image_element:\n",
    "        image_url = \"https:\" + image_element[\"src\"]\n",
    "        image_filename = os.path.join(\"images\", f\"{year}_{name.replace(' ', '_')}.jpg\")\n",
    "        if download_image(image_url, image_filename):\n",
    "            # Add text to the image\n",
    "            text = f\"{year} - {name}\"\n",
    "            output_filename = os.path.join(\"images\", f\"{year}_{name.replace(' ', '_')}_with_text.jpg\")\n",
    "            add_text_to_image(image_filename, text, output_filename)\n",
    "\n",
    "    # Extract brief bio\n",
    "    bio_element = soup.find(\"p\")\n",
    "    bio = \"\"\n",
    "    if bio_element:\n",
    "        bio = bio_element.text.strip()\n",
    "\n",
    "    # Clean up the bio text\n",
    "    bio = re.sub(r'\\[\\d+\\]', '', bio)  # Remove reference numbers\n",
    "\n",
    "    # Add bio to the titleholder data\n",
    "    titleholder.append(bio)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('miss_universe_titleholders_with_bios.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Year', 'Country', 'Name', 'Age', 'Hometown', 'National Title', 'Date', 'Entrants', 'Page URL', 'Bio'])\n",
    "    writer.writerows(titleholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
